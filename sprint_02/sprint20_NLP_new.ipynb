{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sprint20 自然言語処理 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' は、内部コマンドまたは外部コマンド、\n",
      "操作可能なプログラムまたはバッチ ファイルとして認識されていません。\n"
     ]
    }
   ],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' は、内部コマンドまたは外部コマンド、\n",
      "操作可能なプログラムまたはバッチ ファイルとして認識されていません。\n"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = train_review.data, train_review.target\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : 1\n"
     ]
    }
   ],
   "source": [
    "print(\"x : {}\".format(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset = \\\n",
    "  [\"This movie is very good.\",\n",
    "  \"This film is a good\",\n",
    "  \"Very bad. Very, very bad.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  is  movie  this  very\n",
       "0  0    0     0     1   1      1     1     1\n",
       "1  1    0     1     1   1      0     1     0\n",
       "2  0    2     0     0   0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "# DataFrameにまとめる\n",
    "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストはBoWにより各サンプルが語彙数の次元を持つ特徴量となり、機械学習モデルへ入力できるようになります。  \n",
    "この時使用したテキスト全体のことを コーパス と呼びます。  \n",
    "語彙はコーパスに含まれる言葉よって決まり、それを特徴量としてモデルの学習を行います。  \n",
    "そのため、テストデータではじめて登場する語彙はベクトル化される際に無視されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoWは厳密には単語を数えているのではなく、 トークン（token） として定めた固まりを数えます。  \n",
    "何をトークンとするかはCountVectorizerでは引数token_patternで 正規表現 の記法により指定されます。  \n",
    "デフォルトはr'(?u)\\b\\w\\w+\\b'ですが、上の例ではr'(?u)\\b\\w+\\b'としています。  \n",
    "デフォルトでは空白・句読点・スラッシュなどに囲まれた2文字以上の文字を1つのトークンとして抜き出すようになっているため、\n",
    "「a」や「I」などがカウントされません。  \n",
    "英語では1文字の単語は文章の特徴をあまり表さないため、除外されることもあります。  \n",
    "しかし、上の例では1文字の単語もトークンとして抜き出すように引数を指定しています。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a good</th>\n",
       "      <th>bad very</th>\n",
       "      <th>film is</th>\n",
       "      <th>is a</th>\n",
       "      <th>is very</th>\n",
       "      <th>movie is</th>\n",
       "      <th>this film</th>\n",
       "      <th>this movie</th>\n",
       "      <th>very bad</th>\n",
       "      <th>very good</th>\n",
       "      <th>very very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a good  bad very  film is  is a  is very  movie is  this film  this movie  \\\n",
       "0       0         0        0     0        1         1          0           1   \n",
       "1       1         0        1     1        0         0          1           0   \n",
       "2       0         1        0     0        0         0          0           0   \n",
       "\n",
       "   very bad  very good  very very  \n",
       "0         0          1          0  \n",
       "1         0          0          0  \n",
       "2         2          0          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ngram_rangeで利用するn-gramの範囲を指定する\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題１】 BoWのスクラッチ実装（unigram & 2_gram）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_dataset = \\\n",
    "[\"This movie is SOOOO funny!!!\",\n",
    "\"What a movie! I never\",\n",
    "\"best movie ever!!!!! this movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column list を作成する。\n",
    "# index に合わせて、カウントしていく。\n",
    "\n",
    "get_columns = []\n",
    "\n",
    "for str1 in bow_dataset:\n",
    "    temp = str1.replace(\"!\",\"\")\n",
    "    temp = temp.split(\" \")\n",
    "    for wrd in temp:\n",
    "        if wrd in get_columns:\n",
    "            pass\n",
    "        else:\n",
    "            get_columns.append(wrd)\n",
    "\n",
    "df = pd.DataFrame(index=range(len(bow_dataset)),columns=get_columns)\n",
    "\n",
    "for i,str1 in enumerate(bow_dataset):\n",
    "    temp = str1.replace(\"!\",\"\")\n",
    "    temp = temp.split(\" \")\n",
    "    df.iloc[i] = 0\n",
    "    for wrd in temp:\n",
    "        df.iloc[i][wrd]  += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>SOOOO</th>\n",
       "      <th>funny</th>\n",
       "      <th>What</th>\n",
       "      <th>a</th>\n",
       "      <th>I</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   This  movie  is  SOOOO  funny  What  a  I  never  best  ever  this\n",
       "0     1      1   1      1      1     0  0  0      0     0     0     0\n",
       "1     0      1   0      0      0     1  1  1      1     0     0     0\n",
       "2     0      2   0      0      0     0  0  0      0     1     1     1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column list を作成する。\n",
    "# index に合わせて、カウントしていく。\n",
    "\n",
    "ngram=2\n",
    "get_columns = []\n",
    "for str1 in bow_dataset:\n",
    "    temp = str1.replace(\"!\",\"\")\n",
    "    temp = temp.split(\" \")\n",
    "    for i in range(len(temp) - ngram + 1):\n",
    "        \n",
    "        ngram_word =\" \".join(temp[i:i+ngram])\n",
    "        if ngram_word not in get_columns:\n",
    "            get_columns.append(ngram_word)\n",
    "\n",
    "df = pd.DataFrame(index=range(len(bow_dataset)),columns=get_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,str1 in enumerate(bow_dataset):\n",
    "    temp = str1.replace(\"!\",\"\")\n",
    "    temp = temp.split(\" \")\n",
    "    df.iloc[i] = 0\n",
    "    for j in range(len(temp)-ngram + 1):\n",
    "        ngram_word = \" \".join(temp[j:j+ngram])\n",
    "        df.iloc[i][ngram_word]  += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This movie</th>\n",
       "      <th>movie is</th>\n",
       "      <th>is SOOOO</th>\n",
       "      <th>SOOOO funny</th>\n",
       "      <th>What a</th>\n",
       "      <th>a movie</th>\n",
       "      <th>movie I</th>\n",
       "      <th>I never</th>\n",
       "      <th>best movie</th>\n",
       "      <th>movie ever</th>\n",
       "      <th>ever this</th>\n",
       "      <th>this movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   This movie  movie is  is SOOOO  SOOOO funny  What a  a movie  movie I  \\\n",
       "0           1         1         1            1       0        0        0   \n",
       "1           0         0         0            0       1        1        1   \n",
       "2           0         0         0            0       0        0        0   \n",
       "\n",
       "   I never  best movie  movie ever  ever this  this movie  \n",
       "0        0           0           0          0           0  \n",
       "1        1           0           0          0           0  \n",
       "2        0           1           1          1           1  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】TF-IDFの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hirot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = x_train\n",
    "vectorizer = TfidfVectorizer(stop_words=stopWords,token_pattern=r'(?u)\\b\\w+\\b', max_features=50)\n",
    "\n",
    "# norm = Noneといれることで正則化が行われなくなる。\n",
    "\n",
    "# fit することでvocabluaryが設定される。\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "#vocab = vectorizer.vocabulary_\n",
    "\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "#X = vectorizer.fit_transform(corpus).toarray()\n",
    "# toarray()で、スパース表現→配列表現にできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.12785745, 0.39821412, 0.        , 0.43800216,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.22692136, 0.        , 0.        , 0.        , 0.12816108,\n",
       "        0.12526271, 0.10609775, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.130662  ,\n",
       "        0.        , 0.42312832, 0.32387604, 0.13799925, 0.2456185 ,\n",
       "        0.        , 0.17039113, 0.        , 0.14138692, 0.23765553,\n",
       "        0.        , 0.        , 0.        , 0.11557921, 0.        ,\n",
       "        0.16340563, 0.        , 0.        , 0.        , 0.11146519,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＜メモ＞  \n",
    "stop_wordsを使用することで、英語で使い古される単語をfeatures に含めないようにする。  \n",
    "r(?u)\\b\\n+\\b　ｒは正規化、「raw_string」。  \n",
    "?u は、unicodeを探索せよという意味、\\bは空文字を探索せよ、\\wはストリング、  \n",
    "＋は、直前のパターン（この場合はストリング）を１回以上繰り返すという意味である。すなわち、  \n",
    "\" abcd \"のような表現を探索せよという意味である。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】TF-IDFを用いた学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = TfidfVectorizer(stop_words=stopWords,token_pattern=r'(?u)\\b\\w+\\b', max_features=5000)\n",
    "X=vectorizer1.fit_transform(corpus)\n",
    "vocab = vectorizer1.vocabulary_\n",
    "\n",
    "vectorizer2 = TfidfVectorizer(stop_words=stopWords,token_pattern=r'(?u)\\b\\w+\\b', max_features=5000,vocabulary=vocab)\n",
    "X_test = vectorizer2.fit_transform(x_test)\n",
    "\n",
    "##　vocab　により、フィット時に選択された5000の単語による特徴量をテスト用データに流用させることで予測精度を向上させている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "Y = y_train\n",
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(X, Y)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88176"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　2_gramの場合　#\n",
    "vectorizer1_ng2 = TfidfVectorizer(stop_words=stopWords, ngram_range=(2,2),token_pattern=r'(?u)\\b\\w+\\b', max_features=5000)\n",
    "X=vectorizer1_ng2.fit_transform(corpus)\n",
    "vocab2 = vectorizer1_ng2.vocabulary_\n",
    "\n",
    "vectorizer2_ng2 = TfidfVectorizer(stop_words=stopWords, ngram_range=(2,2),token_pattern=r'(?u)\\b\\w+\\b', max_features=5000,\n",
    "                                 vocabulary=vocab2)\n",
    "X_test = vectorizer2_ng2.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "Y = y_train\n",
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(X, Y)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81496"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2-gramにすると、精度が落ちた\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】TF-IDFのスクラッチ実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This movie</th>\n",
       "      <th>movie is</th>\n",
       "      <th>is SOOOO</th>\n",
       "      <th>SOOOO funny</th>\n",
       "      <th>What a</th>\n",
       "      <th>a movie</th>\n",
       "      <th>movie I</th>\n",
       "      <th>I never</th>\n",
       "      <th>best movie</th>\n",
       "      <th>movie ever</th>\n",
       "      <th>ever this</th>\n",
       "      <th>this movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.274653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   This movie  movie is  is SOOOO  SOOOO funny    What a   a movie   movie I  \\\n",
       "0    0.274653  0.274653  0.274653     0.274653  0.000000  0.000000  0.000000   \n",
       "1    0.000000  0.000000  0.000000     0.000000  0.274653  0.274653  0.274653   \n",
       "2    0.000000  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    I never  best movie  movie ever  ever this  this movie  \n",
       "0  0.000000    0.000000    0.000000   0.000000    0.000000  \n",
       "1  0.274653    0.000000    0.000000   0.000000    0.000000  \n",
       "2  0.000000    0.274653    0.274653   0.274653    0.274653  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX = df\n",
    "\n",
    "sample = df.shape[0]\n",
    "for i in range(df.shape[0]):\n",
    "    all_token = np.sum(df.loc[i,:])\n",
    "    for str in df.columns:\n",
    "        XX.loc[i,str] = df.loc[i,str]/all_token * np.log(sample/(df[str]>0).sum())\n",
    "        \n",
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This movie</th>\n",
       "      <th>movie is</th>\n",
       "      <th>is SOOOO</th>\n",
       "      <th>SOOOO funny</th>\n",
       "      <th>What a</th>\n",
       "      <th>a movie</th>\n",
       "      <th>movie I</th>\n",
       "      <th>I never</th>\n",
       "      <th>best movie</th>\n",
       "      <th>movie ever</th>\n",
       "      <th>ever this</th>\n",
       "      <th>this movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.301737</td>\n",
       "      <td>0.301737</td>\n",
       "      <td>0.301737</td>\n",
       "      <td>0.301737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301737</td>\n",
       "      <td>0.301737</td>\n",
       "      <td>0.301737</td>\n",
       "      <td>0.301737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301737</td>\n",
       "      <td>0.301737</td>\n",
       "      <td>0.301737</td>\n",
       "      <td>0.301737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   This movie  movie is  is SOOOO  SOOOO funny    What a   a movie   movie I  \\\n",
       "0    0.301737  0.301737  0.301737     0.301737  0.000000  0.000000  0.000000   \n",
       "1    0.000000  0.000000  0.000000     0.000000  0.301737  0.301737  0.301737   \n",
       "2    0.000000  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    I never  best movie  movie ever  ever this  this movie  \n",
       "0  0.000000    0.000000    0.000000   0.000000    0.000000  \n",
       "1  0.301737    0.000000    0.000000   0.000000    0.000000  \n",
       "2  0.000000    0.301737    0.301737   0.301737    0.301737  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX1 = df\n",
    "\n",
    "sample = df.shape[0]\n",
    "for i in range(df.shape[0]):\n",
    "    all_token = np.sum(df.loc[i,:])\n",
    "    for str in df.columns:\n",
    "        XX1.loc[i,str] = np.log((1+sample)/(1+(df[str]>0).sum())+1) * df.loc[i,str]\n",
    "  \n",
    "XX1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/09/ed/b59a2edde05b7f5755ea68648487c150c7c742361e9c8733c6d4ca005020/gensim-3.8.1-cp37-cp37m-win_amd64.whl (24.2MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from gensim) (1.3.1)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/6e/14/47cf88d290e4681be35f3b6e8889ba26ed809a0ba14336dc8ae46ffcfda8/smart_open-1.10.0.tar.gz (99kB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from gensim) (1.16.5)\n",
      "Requirement already satisfied: requests in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Collecting boto3 (from smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/4f/3facbb42e8d07db1ef9b8cefb28dd1dbfcd52a8e32a0323d57f59b10e147/boto3-1.12.31-py2.py3-none-any.whl (128kB)\n",
      "Collecting google-cloud-storage (from smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/88/67/80761781f813ffbf8bc1db7270b6d23de7a96468da4601de3bf2e5e1d829/google_cloud_storage-1.26.0-py2.py3-none-any.whl (75kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/43/1e939e1fcd87b827fe192d0c9fc25b48c5b3368902bfb913de7754b0dc03/jmespath-0.9.5-py2.py3-none-any.whl\n",
      "Collecting botocore<1.16.0,>=1.15.31 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/e0/07/8ac3fe64e3a245c72c61688dc37e89f0c16f727eec92252733535794f9c3/botocore-1.15.31-py2.py3-none-any.whl (6.0MB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "Collecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/9e/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098/google_resumable_media-0.5.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-core<2.0dev,>=1.2.0 (from google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/89/3c/8a7531839028c9690e6d14c650521f3bbaf26e53baaeb2784b8c3eb2fb97/google_cloud_core-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (1.11.2)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.31->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.31->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Collecting google-api-core<2.0.0dev,>=1.16.0 (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/63/7e/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (41.4.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (3.1.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (2019.3)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (3.11.4)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/46/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf/googleapis-common-protos-1.51.0.tar.gz\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\hirot\\anaconda3\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.4.8)\n",
      "Building wheels for collected packages: smart-open, googleapis-common-protos\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-1.10.0-cp37-none-any.whl size=90638 sha256=2db4623b66edacad7ac1f8e11fbe8768bb74f08bd5bf3046586f2e3cb71c388f\n",
      "  Stored in directory: C:\\Users\\hirot\\AppData\\Local\\pip\\Cache\\wheels\\f8\\00\\d4\\a6b8b6aa241459103d19e757f96645549dd562d5b5de653f44\n",
      "  Building wheel for googleapis-common-protos (setup.py): started\n",
      "  Building wheel for googleapis-common-protos (setup.py): finished with status 'done'\n",
      "  Created wheel for googleapis-common-protos: filename=googleapis_common_protos-1.51.0-cp37-none-any.whl size=77603 sha256=93440ee1a782f2a126f930e703f8a55245491c975f0d85c5e2fba1fd5365a534\n",
      "  Stored in directory: C:\\Users\\hirot\\AppData\\Local\\pip\\Cache\\wheels\\2c\\f9\\7f\\6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\n",
      "Successfully built smart-open googleapis-common-protos\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, google-resumable-media, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-storage, smart-open, gensim\n",
      "Successfully installed boto3-1.12.31 botocore-1.15.31 gensim-3.8.1 google-api-core-1.16.0 google-cloud-core-1.3.0 google-cloud-storage-1.26.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jmespath-0.9.5 s3transfer-0.3.3 smart-open-1.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['this', 'movie', 'is', 'very', 'good', 'film', 'a', 'bad'])\n",
      "thisのベクトル : \n",
      "[-0.02098105  0.02258059  0.02851041 -0.00124121  0.03891818  0.04506667\n",
      " -0.0109305   0.01840642  0.02327806 -0.00697161]\n",
      "movieのベクトル : \n",
      "[-0.00066643  0.02399119 -0.02114978  0.028674    0.03904394  0.03756126\n",
      "  0.04605712  0.04721591 -0.03711565  0.04108435]\n",
      "isのベクトル : \n",
      "[ 0.03116227 -0.04661803  0.01974387  0.01425194  0.04564355  0.01820397\n",
      " -0.02511046  0.02846979  0.04203334 -0.03212313]\n",
      "veryのベクトル : \n",
      "[-0.04773384 -0.00348218 -0.01315404  0.00433041 -0.00785528  0.01832622\n",
      " -0.03504654 -0.03432597  0.03548326 -0.04261057]\n",
      "goodのベクトル : \n",
      "[-0.03624356  0.01948744 -0.00224068 -0.01728424 -0.00658405  0.03463987\n",
      "  0.03276391 -0.01304061 -0.0311131  -0.04174571]\n",
      "filmのベクトル : \n",
      "[ 0.0390854   0.03092853 -0.03155345  0.03109247 -0.00609201 -0.02193109\n",
      "  0.01128496 -0.01367472  0.01945359 -0.03267679]\n",
      "aのベクトル : \n",
      "[ 0.0478286  -0.00469719  0.0167172   0.00841532 -0.03170535  0.02022313\n",
      "  0.01855317  0.02373713 -0.012665   -0.03285693]\n",
      "badのベクトル : \n",
      "[ 0.03910708  0.01262187 -0.01438209  0.03942366 -0.01809485 -0.04916563\n",
      "  0.01088991 -0.00505487  0.0217884  -0.03584883]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hirot\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
    "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "for vocab in model.wv.vocab.keys():\n",
    "  print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('very', 0.29364627599716187),\n",
       " ('this', 0.2116437554359436),\n",
       " ('movie', 0.1516377478837967)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"good\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hirot\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "vocabs = model.wv.vocab.keys()\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】コーパスの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ストップワードの定義\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append(\"www\")\n",
    "stop_words.append(\"br\")\n",
    "stop_words.append(\"http\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　前処理：全体を小文字にするlower、正規表現で文章構成要素を分解、リスト内包の繰り返し処理で\"\"を排除、ストップワードの排除。\n",
    "#　lowerで処理が停止する場合は、上記のｘ_trainなどを再定義する。\n",
    "\n",
    "pp_x_train = x_train\n",
    "for sample_num in range(len(x_train)):\n",
    "    temp = x_train[sample_num].lower()\n",
    "    temp = re.findall(r\"(?u)\\b\\w\\w+\\b\",temp)\n",
    "    temp = [temp[i*2] for i in range(int(len(temp)/2))]\n",
    "    temp = [word for word in temp if word not in stop_words]\n",
    "    pp_x_train[sample_num] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】Word2Vecの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hirot\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7191605, 7472095)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vecによる学習。diverとほぼ同文。計算処理負荷、2値分類を行うため特徴量を30とした。\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(min_count=1, size=30) # 次元数を30に設定\n",
    "model.build_vocab(pp_x_train) # 準備\n",
    "model.train(pp_x_train, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
    "#print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "#for vocab in model.wv.vocab.keys():\n",
    "#   print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47412086,  0.5733074 , -0.38808358,  0.4709022 ,  0.42264494,\n",
       "       -1.3199958 ,  0.6007901 ,  1.1138368 ,  0.03360257, -0.17457487,\n",
       "       -0.6862977 , -1.9612445 , -2.5635514 , -0.2549451 ,  1.0476553 ,\n",
       "       -1.4398404 , -1.2268697 ,  0.47406617,  1.081101  , -0.65287375,\n",
       "       -0.16128549,  0.39934334, -0.17143781,  0.05266453,  0.85679615,\n",
       "       -0.93853444, -2.3548982 , -0.06048483,  1.8899999 , -1.0690849 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#　適当な単語の分散表現\n",
    "model.wv[\"like\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】Word2Vecの学習結果（分散表現）を用いた文書分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 入力：前処理されたトレイン、テストデータ。出力：30特徴量による分散表現。\n",
    "\n",
    "def make_all(pp_sentense):\n",
    "\n",
    "    data_feature = np.zeros((len(pp_sentense),30))\n",
    "\n",
    "    for i in range(len(pp_sentense)):\n",
    "        data_feature[i] = make_data1(pp_sentense[i])\n",
    "    return data_feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "### shapeの問題で処理がなぜか止まるので、make_all関数内でmake_data1関数を繰り返し実行する構造にしている。\n",
    "### 入力：前処理されたトレイン、テストデータの１サンプル。出力：30特徴量で分散表現された1サンプル。\n",
    "###　今回の特徴量は、1サンプル内の全単語の分散表現の合計値を正規化したものとした。\n",
    "\n",
    "def make_data1(pp_sentense):\n",
    "    sentense_features = np.zeros(30)\n",
    "    for word in pp_sentense:\n",
    "        sentense_features += model.wv[word]\n",
    "\n",
    "    data_feature = scaler.fit_transform(sentense_features.reshape(-1,1))\n",
    "    return data_feature.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.78899095,  0.99042181, -1.17629177,  1.23811069,  0.14526573,\n",
       "        -1.47146235,  0.49844051,  2.28492101, -0.35230013, -0.06783827,\n",
       "        -0.81375075, -1.45007181, -1.18424343, -0.62499454,  1.7571657 ,\n",
       "        -1.02968619, -0.80964854, -0.59406694,  0.75027452, -1.33023401,\n",
       "         0.06132184,  0.61822049,  0.78033367,  0.54698898,  0.27888579,\n",
       "        -0.49643085, -1.46692682,  0.36418719,  1.19006275,  0.57435479],\n",
       "       [ 0.57161173,  0.99331906, -0.941552  ,  1.38521073,  0.48289026,\n",
       "        -1.58383487,  0.42121845,  2.10278855, -0.53610177, -0.34558234,\n",
       "        -0.90759965, -1.56623046, -0.97367932,  0.20619229,  1.93196708,\n",
       "        -0.99420177, -1.05356566, -0.29996199,  1.2809279 , -0.76097884,\n",
       "         0.01359021,  0.55740344,  1.11624002,  0.34824756,  0.02087206,\n",
       "        -0.7712616 , -1.42721606,  0.31398244,  0.99193411, -0.57662953]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_all(pp_x_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "##　テストデータの単語がトレインデータの単語に含まれないケースがある。トレインデータの単語に含まれない単語を排除した（加筆箇所）。\n",
    "\n",
    "pp_x_test = x_test\n",
    "for sample_num in range(len(x_test)):\n",
    "    temp = x_test[sample_num].lower()\n",
    "    temp = re.findall(r\"(?u)\\b\\w\\w+\\b\",temp)\n",
    "    temp = [temp[i*2] for i in range(int(len(temp)/2))]\n",
    "    temp = [word for word in temp if word not in stop_words]\n",
    "    temp = [word for word in temp if word in model.wv.vocab.keys()]    ##　加筆　##\n",
    "    pp_x_test[sample_num] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##前処理されたトレインデータ、テストデータをmake_allで分散表現に変換しＳＶＣで２値分類を行った。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traindata = make_all(pp_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testdata = make_all(pp_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf2 = SVC(gamma='auto')\n",
    "svm_w2v = clf2.fit(X_traindata, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_w2v.predict(X_testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76328"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
